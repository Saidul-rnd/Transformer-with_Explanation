{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "8Tdbz3Sbe63R",
        "outputId": "55487825-95e0-41ae-fbd0-47792324dc44"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Self Attention mechanism works with 3(Three) vectors, Query (Q), Key(K) and Value(V)\\nQuery(Q): what I am looking for?\\nKey(K): What I can offer?\\nValue(V): What I actually offer? '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "'''\n",
        "Self Attention mechanism works with 3(Three) vectors, Query (Q), Key(K) and Value(V)\n",
        "Query(Q): what I am looking for?\n",
        "Key(K): What I can offer?\n",
        "Value(V): What I actually offer? '''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self Attention\n",
        "\n",
        "$$\n",
        "\\text{self attention} = softmax\\bigg(\\frac{Q.K^T}{\\sqrt{d_k}}+M\\bigg)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{new V} = \\text{self attention}.V\n",
        "$$"
      ],
      "metadata": {
        "id": "1ns-wNaNomSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math"
      ],
      "metadata": {
        "id": "g71bC3mwfu5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we are randomly creating some value for Q, K, V.\n",
        "\n",
        "L , d_k, d_v = 4, 8, 8 # L is lenth of my input sequence, so here four words\n",
        "\n",
        "'''Setting d_k, the same for Query and Key is a common practice\n",
        "because it allows the dot product operation to be well-defined.\n",
        "The dot product of two vectors is mathematically defined when\n",
        "they have the same dimensionality. This symmetry ensures that\n",
        "the attention mechanism can effectively capture relationships\n",
        "between elements in the input sequence.'''\n",
        "\n",
        "Q = np.random.randn(L,d_k) #row X column\n",
        "K = np.random.randn(L,d_k)\n",
        "V = np.random.randn(L,d_v)\n",
        "\n",
        "print(\"Q\\n\", Q)\n",
        "print(\"K\\n\", K)\n",
        "print(\"V\\n\", V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8mN4SuSf-a3",
        "outputId": "843ab604-9875-4389-9b7e-11ea68a66bfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q\n",
            " [[ 1.71085486 -0.03367367  1.09812443  1.09528995 -0.39000119 -0.45841432\n",
            "  -0.48539386  1.5718894 ]\n",
            " [ 0.00965999  0.54801593  0.75797904  0.67755443  1.90984137  1.62987475\n",
            "  -0.33656417  1.08604924]\n",
            " [ 0.12680977 -0.47276565 -1.48679567  0.19848653  0.19646147 -0.01184547\n",
            "   1.69037273  0.47946585]\n",
            " [ 0.57315114 -0.63954299 -1.77893003 -0.22080153  0.34005573  0.51011648\n",
            "   0.15552844  0.10861496]]\n",
            "K\n",
            " [[-0.59194089 -0.81207564  1.46555827 -1.79502953  0.48312732 -0.86651849\n",
            "  -1.86632717 -0.77304799]\n",
            " [ 0.83096345 -1.55802823  0.44207751  0.17283549 -0.4557931   0.51542019\n",
            "   0.20530228  1.55508918]\n",
            " [ 2.32427521 -0.04541923  2.12979551 -0.72525274 -0.36016954 -0.771219\n",
            "   1.25401108 -0.17451855]\n",
            " [-2.03744382  1.8421055   0.91399836 -0.97696761 -1.67673923  0.66197614\n",
            "   0.34689897 -1.02507032]]\n",
            "V\n",
            " [[-0.876697   -1.33901344 -0.77828713  0.00691392 -1.23706158 -1.20140183\n",
            "  -0.12382458 -1.19784545]\n",
            " [ 0.09513252  0.94729865 -0.34992144  0.72716694  0.80692249  0.76361469\n",
            "   1.71665611 -0.29361123]\n",
            " [-1.18177799 -0.10052465  0.54524521 -1.23967226  0.5490008   0.40119133\n",
            "  -1.66218324  0.0878174 ]\n",
            " [-0.1657854  -0.65472211 -0.23727649 -1.25247583  0.98382351  1.45022557\n",
            "   1.08493157 -0.04920551]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''The output above we got, each of the vector in Q, K & V is representing\n",
        "each of the word with 8 X 1 vector.\n",
        "For example,if we consider \"I live in Montreal\" sentence(L=4), for the word \"i\", we got the first vector\n",
        "for Q [ 1.71085486 -0.03367367  1.09812443  1.09528995 -0.39000119 -0.45841432\n",
        "  -0.48539386  1.5718894 ], K [-0.59194089 -0.81207564  1.46555827 -1.79502953  0.48312732 -0.86651849\n",
        "  -1.86632717 -0.77304799] & V [-0.876697   -1.33901344 -0.77828713  0.00691392 -1.23706158 -1.20140183\n",
        "  -0.12382458 -1.19784545]. SO the first vector of the matrix is in Q, K, V for \"i\"."
      ],
      "metadata": {
        "id": "KcvsyAHYhU-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" we calculated the focus of each word on which other word,\n",
        "mean, calculating the attention score. For example, for the first vector below,\n",
        "the first word is focusing on the third word as in the first vector the third value\n",
        "is highest 5.133\"\"\"\n",
        "\n",
        "np.matmul(Q, K.T) # key - Transposed to match the matmul shape."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdiwyRO3knQp",
        "outputId": "4761a1a8-b3fe-4e9c-9b7f-e6a08bfc671e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.44252979,  4.43514305,  5.13343669, -5.04338883],\n",
              "       [-1.25716568,  1.1957776 , -1.43594383, -2.33273404],\n",
              "       [-5.64667572,  1.21598145, -1.01986374, -2.92444943],\n",
              "       [-2.68266133,  0.95687194, -2.60722193, -4.04597725]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Why do we need the denominator part in the formula root\"dk\",\n",
        "\"\"\"\" we need this becuase this helps to reduce the variance of Q & K.T \"\"\n",
        "lets check, the variance of 'np.matmul(Q, K.T).var()' is so high and not in same range\"\"\"\n",
        "\n",
        "Q.var(), K.var(), np.matmul(Q, K.T).var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "io94OG13mqXS",
        "outputId": "29fb7f8b-c904-4ed1-eae1-6453f8a7a410"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7526859311070544, 1.3816413704695116, 8.696591881898748)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" so lets devide by root of dk and\n",
        " check the variance of scaled which pretty low than before and sam range\"\"\"\n",
        "scaled = np.matmul(Q, K.T) / math.sqrt(d_k)\n",
        "#check the variance now\n",
        "Q.var(), K.var(), scaled.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFZ8aEsxoP6r",
        "outputId": "eb77eb2b-ac41-4de0-ce77-b789475229dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7526859311070544, 1.3816413704695116, 1.0870739852373434)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaled"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q--Oc66fpOeP",
        "outputId": "151259d5-330d-4dd9-ca19-eaa0a3aefaf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.5100113 ,  1.56805986,  1.81494395, -1.78310722],\n",
              "       [-0.44447519,  0.42277123, -0.50768281, -0.82474603],\n",
              "       [-1.99640135,  0.42991437, -0.36057628, -1.03394901],\n",
              "       [-0.94846401,  0.33830532, -0.92179215, -1.43046898]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Masking**\n",
        ">Masking is needed for the decoder part of transformer, not for encoder.\n",
        ">Its needed to ensure present words dont get context from the words to be generated in the future."
      ],
      "metadata": {
        "id": "r-HkS04Jpk5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"here we can see the first word only can see itself, can see next words\n",
        "simillary for the other words as well\"\"\"\n",
        "mask = np.tril(np.ones((L,L))) # created a triange martix\n",
        "mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgvGS7eypl6Y",
        "outputId": "c121142e-ce88-402e-8029-e5510cb73f91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0.],\n",
              "       [1., 1., 0., 0.],\n",
              "       [1., 1., 1., 0.],\n",
              "       [1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" here we are placing the zero in place of 1 and -infinity in case of 0,\n",
        "becuase, we want same value as scaled in case of 1 value and no information\n",
        "in case of 0, as we will apply the softmax then the -inf will be become zero\"\"\"\n",
        "mask[mask == 0] = -np.infty\n",
        "mask[mask == 1] = 0"
      ],
      "metadata": {
        "id": "HjurgYdzqQAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHU1fqD-q0X7",
        "outputId": "358905bd-b469-4cac-9413-e6797cc3b8b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0., -inf, -inf, -inf],\n",
              "       [  0.,   0., -inf, -inf],\n",
              "       [  0.,   0.,   0., -inf],\n",
              "       [  0.,   0.,   0.,   0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaled+mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raKwPefOsCPj",
        "outputId": "61d02a80-b228-4c4d-baf4-15386b9cac9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.5100113 ,        -inf,        -inf,        -inf],\n",
              "       [-0.44447519,  0.42277123,        -inf,        -inf],\n",
              "       [-1.99640135,  0.42991437, -0.36057628,        -inf],\n",
              "       [-0.94846401,  0.33830532, -0.92179215, -1.43046898]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Softmax\n",
        "\n",
        "$$\n",
        "\\text{softmax} = \\frac{e^{x_i}}{\\sum_j e^x_j}\n",
        "$$"
      ],
      "metadata": {
        "id": "9nAmoZHctQsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  return (np.exp(x).T / np.sum(np.exp(x), axis = -1)).T"
      ],
      "metadata": {
        "id": "5UBsMiEIsFFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" As the softmax explain as probability, so sum of each row will be 1\"\"\"\n",
        "attention = softmax(scaled)\n",
        "attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfavnKxwsslB",
        "outputId": "01c92a01-b3d6-452b-e85a-9d44a5ff3319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.0512946 , 0.40979482, 0.52454996, 0.01436062],\n",
              "       [0.19988918, 0.47580567, 0.18764568, 0.13665948],\n",
              "       [0.04982831, 0.56391251, 0.25580322, 0.13045596],\n",
              "       [0.15960052, 0.57792451, 0.16391464, 0.09856034]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" As the softmax explain as probability, so sum of each row will be 1\"\"\"\n",
        "attention_mask = softmax(scaled+mask)\n",
        "attention_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQgZqLkwsS1F",
        "outputId": "02d96b18-606d-439d-c779-473384b11af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.        , 0.        , 0.        ],\n",
              "       [0.29582759, 0.70417241, 0.        , 0.        ],\n",
              "       [0.05730396, 0.64851518, 0.29418086, 0.        ],\n",
              "       [0.15960052, 0.57792451, 0.16391464, 0.09856034]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_v = np.matmul(attention, V)\n",
        "new_v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWZXSXq8sZHp",
        "outputId": "1dfb75f7-9336-48e8-b753-527925a32bff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.62826739,  0.25738151,  0.099283  , -0.36991247,  0.56932474,\n",
              "         0.48257096, -0.15919262, -0.13640537],\n",
              "       [-0.37438932,  0.07473878, -0.25217896, -0.05640966,  0.37412951,\n",
              "         0.39665387,  0.6284082 , -0.36938407],\n",
              "       [-0.31396822,  0.35634584, -0.12758447, -0.07010205,  0.6621748 ,\n",
              "         0.66256466,  0.67821785, -0.20921284],\n",
              "       [-0.29499245,  0.25275278, -0.26045559,  0.09470619,  0.45585986,\n",
              "         0.45826315,  0.80681023, -0.35131703]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "  d_k = Q.shape[-1]\n",
        "  scaled = np.matmul(Q, K.T) / math.sqrt(d_k)\n",
        "  if mask is not None:\n",
        "    scaled = scaled + mask\n",
        "  attention = softmax(scaled)\n",
        "  out = np.matmul(attention, V)\n",
        "  return out, attention"
      ],
      "metadata": {
        "id": "FebAoXNss5SJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values, attention = scaled_dot_product_attention(Q, K, V, mask=mask)\n",
        "print(\"Q\\n\", Q)\n",
        "print(\"K\\n\", K)\n",
        "print(\"V\\n\", V)\n",
        "print(\"New_v\\n\", values)\n",
        "print(\"Attention\\n\", attention)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcU9ekD6tb6B",
        "outputId": "509687c8-dbb0-44e3-c0a0-844a399ab044"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q\n",
            " [[ 1.71085486 -0.03367367  1.09812443  1.09528995 -0.39000119 -0.45841432\n",
            "  -0.48539386  1.5718894 ]\n",
            " [ 0.00965999  0.54801593  0.75797904  0.67755443  1.90984137  1.62987475\n",
            "  -0.33656417  1.08604924]\n",
            " [ 0.12680977 -0.47276565 -1.48679567  0.19848653  0.19646147 -0.01184547\n",
            "   1.69037273  0.47946585]\n",
            " [ 0.57315114 -0.63954299 -1.77893003 -0.22080153  0.34005573  0.51011648\n",
            "   0.15552844  0.10861496]]\n",
            "K\n",
            " [[-0.59194089 -0.81207564  1.46555827 -1.79502953  0.48312732 -0.86651849\n",
            "  -1.86632717 -0.77304799]\n",
            " [ 0.83096345 -1.55802823  0.44207751  0.17283549 -0.4557931   0.51542019\n",
            "   0.20530228  1.55508918]\n",
            " [ 2.32427521 -0.04541923  2.12979551 -0.72525274 -0.36016954 -0.771219\n",
            "   1.25401108 -0.17451855]\n",
            " [-2.03744382  1.8421055   0.91399836 -0.97696761 -1.67673923  0.66197614\n",
            "   0.34689897 -1.02507032]]\n",
            "V\n",
            " [[-0.876697   -1.33901344 -0.77828713  0.00691392 -1.23706158 -1.20140183\n",
            "  -0.12382458 -1.19784545]\n",
            " [ 0.09513252  0.94729865 -0.34992144  0.72716694  0.80692249  0.76361469\n",
            "   1.71665611 -0.29361123]\n",
            " [-1.18177799 -0.10052465  0.54524521 -1.23967226  0.5490008   0.40119133\n",
            "  -1.66218324  0.0878174 ]\n",
            " [-0.1657854  -0.65472211 -0.23727649 -1.25247583  0.98382351  1.45022557\n",
            "   1.08493157 -0.04920551]]\n",
            "New_v\n",
            " [[-0.876697   -1.33901344 -0.77828713  0.00691392 -1.23706158 -1.20140183\n",
            "  -0.12382458 -1.19784545]\n",
            " [-0.19236146  0.27094446 -0.47664383  0.51409623  0.20225561  0.18230859\n",
            "   1.17219114 -0.56110866]\n",
            " [-0.33619979  0.50803436 -0.11112759  0.10728714  0.61391848  0.54439345\n",
            "   0.6171994  -0.23321842]\n",
            " [-0.29499245  0.25275278 -0.26045559  0.09470619  0.45585986  0.45826315\n",
            "   0.80681023 -0.35131703]]\n",
            "Attention\n",
            " [[1.         0.         0.         0.        ]\n",
            " [0.29582759 0.70417241 0.         0.        ]\n",
            " [0.05730396 0.64851518 0.29418086 0.        ]\n",
            " [0.15960052 0.57792451 0.16391464 0.09856034]]\n"
          ]
        }
      ]
    }
  ]
}