{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8L9IiA356Wb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\" Sequence length = maximum number of characters/words we can pass into the transformer at a time.\n",
        "Dimension of embedding = size of vector representing each character / word.\"\"\"\n",
        "\n",
        "sequence_length = 4 ## length of the input, in out case (I live in MTL)\n",
        "batch_size = 1 ## batch is for parallal processing.\n",
        "input_dim = 512 ## this is size of vector that represents each of the sequence/word\n",
        "d_model = 512 ## output size of attention unit of every single word\n",
        "\"\"\"# here the logic is, first place the batch_Size, then length of our sequence,\n",
        "# then check how many vectors are representing each words.\"\"\"\n",
        "x = torch.randn(batch_size, sequence_length, input_dim) # randomly generated data. As we did not use positional encoding yet."
      ],
      "metadata": {
        "id": "vbOp1nT57oJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlBgVPAj77qW",
        "outputId": "273d8bab-5cdf-448b-fa3e-bb330d95ffd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function Tensor.size>"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Here, we have to create query, key and values thats why we multiply 3 times the d_model.\"\"\"\n",
        "qkv_layer = nn.Linear(input_dim, 3 * d_model)"
      ],
      "metadata": {
        "id": "AhhYYRrC8ynq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" now pass our input through the q,k,v generator \"\"\"\n",
        "qkv = qkv_layer(x)"
      ],
      "metadata": {
        "id": "0qliE-fG9swM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qkv.shape ## here it means, 1 batch, 4 sequence length and 512x3 = 1536 vector for each word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03r51D3D-bmZ",
        "outputId": "755a43a0-57c5-4390-e264-904154260f9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 1536])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_heads = 8  # number of heads of multi-head attention\n",
        "head_dim = d_model // num_heads # each head dimention would be d_model(512) // num_head (8)\n",
        "\"\"\"  # reshape the qkv, specially the last dimention of the matrix,\n",
        "here last dimention devided into num_heads and 3 x head_dim. here its 3 because, Q, k & V.\"\"\"\n",
        "qkv = qkv.reshape(batch_size, sequence_length, num_heads, 3 * head_dim)"
      ],
      "metadata": {
        "id": "kUL-GouB-hpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qkv.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9A2qnCjv_k70",
        "outputId": "754386a1-717b-4c51-8875-4afb4bab9690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 8, 192])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" we have to permute the 2nd and 3rd dimention switching between sequence length\n",
        "and num_heads, which will be helpful for parallal computation \"\"\"\n",
        "qkv = qkv.permute(0, 2, 1, 3)\n",
        "qkv.shape ## [ batch_size, num_heads, sequence_length, 3* head_dim]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8S2TeUG1A2qw",
        "outputId": "ca7e6d2a-9393-469f-b3a3-b7019d377287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 4, 192])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q, k, v = qkv.chunk(3, dim = -1) # here we created the q, k & v.thatwhy we 3, and using the last dimention so -1\n",
        "q.shape, k.shape, v.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpjEtleiBLwU",
        "outputId": "cb9ca710-770b-48e3-91ce-0c68ddbe4436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 8, 4, 64]),\n",
              " torch.Size([1, 8, 4, 64]),\n",
              " torch.Size([1, 8, 4, 64]))"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self Attention\n",
        "\n",
        "$$\n",
        "\\text{self attention} = softmax\\bigg(\\frac{Q.K^T}{\\sqrt{d_k}}+M\\bigg)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{new V} = \\text{self attention}.V\n",
        "$$"
      ],
      "metadata": {
        "id": "V7mJ0t-LeLQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_k = q.size() [-1] # find the size of the vector dimention,last value\n",
        "\"\"\"Here we applied transpose on key to match the size of tensor with query.\n",
        "usually, we transpose  k.T like this, but here we needed to wrtite k.transpose,\n",
        "becuase this are not matrix, these are tensor. in case of matrix we can transpose\n",
        "k.T like this. Here we mentioned the transpose dimentions, here we want to transpose\n",
        "last two diementions, sequence_length and head_dimention size \"\"\"\n",
        "scaled = torch.matmul(q, k.transpose(-2,-1)) / math.sqrt(d_k)\n",
        "scaled.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KukIkYatdaUz",
        "outputId": "70648aff-17d6-4408-8901-0ddf2a215a2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 4, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "masking, which is needed for decoder."
      ],
      "metadata": {
        "id": "OFd2kTw2kp3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" # we will mask the upcoming words,\n",
        " so placed with -inf that after the softmax func. there is no information.\n",
        " As softmax uses exponance(exp) of each element, so exp of -inf will be zero\"\"\"\n",
        "mask = torch.full(scaled.size(), float('-inf'))\n",
        "mask = torch.triu(mask, diagonal = 1)\n",
        "mask [0] [1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nupNyLVkmc2",
        "outputId": "d156ba98-16e2-4fc8-f5f4-8a6631fb8d06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf, -inf],\n",
              "        [0., 0., -inf, -inf],\n",
              "        [0., 0., 0., -inf],\n",
              "        [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(scaled + mask) [0] [0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5-Hn66qeVjL",
        "outputId": "b3aa3ea1-0ee2-4071-950f-defe15c1f1d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1238,    -inf,    -inf,    -inf],\n",
              "        [ 0.1354,  0.0572,    -inf,    -inf],\n",
              "        [ 0.5056, -0.1130, -0.3738,    -inf],\n",
              "        [ 0.0885,  0.2727,  0.0360,  0.0967]], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# how the softmax works! Just an example\n",
        "np.exp(.2811) / (np.exp(.2811) + np.exp(-0.1152)) #2nd row, prob of 1st ele, is exp of 1st ele / sum of exp of all ele."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yzdYqqUmE09",
        "outputId": "3bcee977-8da7-4fa0-bf7e-e6ff5d9a3855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.597798371649087"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaled += mask"
      ],
      "metadata": {
        "id": "Ko_CUla6lcLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" we want to apply softmax function in the last dimention, which will apply the tensor row by row\"\"\"\n",
        "attention = F.softmax(scaled, dim = -1)"
      ],
      "metadata": {
        "id": "ldWh3wrCl6P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IR6xCkeMo23e",
        "outputId": "4fc084bc-f049-4d3c-b930-7ce4dea527d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5195, 0.4805, 0.0000, 0.0000],\n",
              "        [0.5118, 0.2757, 0.2124, 0.0000],\n",
              "        [0.2404, 0.2890, 0.2281, 0.2424]], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "values = torch.matmul(attention, v)\n",
        "values.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i69haWdPo4ow",
        "outputId": "0f0f7b66-e9f6-4789-dcda-fe2bcdcdb4f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 4, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we have done so far lets make a function for that accordingly."
      ],
      "metadata": {
        "id": "3xYjSCc3pHEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product(q, k , v, mask = None):\n",
        "  d_k = q.size() [-1]\n",
        "  scaled = torch.matmul(q,k.transpose(-1,-2)) / math.sqrt(d_k)\n",
        "  if mask is not None:\n",
        "    scaled +=mask\n",
        "  attention = F.softmax(scaled, dim = -1)\n",
        "  values = torch.matmul(attention, v)\n",
        "  return values, attention"
      ],
      "metadata": {
        "id": "T8cLKUgZpBYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets execute the function\n",
        "values, attention  = scaled_dot_product(q, k, v, mask = None)"
      ],
      "metadata": {
        "id": "um_p-yYBpkLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jfmg2_GBqGIR",
        "outputId": "fef942bb-ab75-43ad-fd34-fabfe28ff76e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 4, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention [0] [0] ## if we make the mask = True then we will se the next values as zero."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHwKUYEKqU31",
        "outputId": "a204ac4e-17be-4a7f-bb43-d2382098cd99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1958, 0.2397, 0.2388, 0.3257],\n",
              "        [0.2661, 0.2461, 0.2624, 0.2254],\n",
              "        [0.4303, 0.2318, 0.1786, 0.1593],\n",
              "        [0.2404, 0.2890, 0.2281, 0.2424]], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "values.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgd6fXqzqYka",
        "outputId": "20fa50f1-a9cb-4c30-9ff1-fd8c04d68b54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 4, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Now we will combine/concanate all the heads all together\"\"\"\n",
        "values = values.reshape(batch_size, sequence_length, num_heads * head_dim) # we multiply the num_head & head_dim together, that we seprated before\n",
        "values.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZhcBQInqyyY",
        "outputId": "84126485-b9c7-4a63-b2ef-1a8f23502ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" as the heads can communicate between each other with the information that they gained,\n",
        "a linear_layer is applied \"\"\"\n",
        "linear_layer = nn.Linear(d_model, d_model)"
      ],
      "metadata": {
        "id": "5wZ2jcIQrHer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = linear_layer(values)"
      ],
      "metadata": {
        "id": "5YDYgykNsAs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.shape ## now this output vector, now much more contex aware and have more informationt han innitial input vector."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6hYl7YbsOKB",
        "outputId": "e35a08ed-d164-44f1-fde1-337d5ab24072"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now lets convert everything we did so far, turn those into Class and function.**"
      ],
      "metadata": {
        "id": "Kronlb43sokv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "# this function we already created above\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scaled += mask\n",
        "    attention = F.softmax(scaled, dim=-1)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.qkv_layer = nn.Linear(input_dim , 3 * d_model)\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, sequence_length, input_dim = x.size()\n",
        "        print(f\"x.size(): {x.size()}\")\n",
        "        qkv = self.qkv_layer(x)\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "        qkv = qkv.permute(0, 2, 1, 3)\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "        print(f\"q size: {q.size()}, k size: {k.size()}, v size: {v.size()}, \")\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        print(f\"values.size(): {values.size()}, attention.size:{ attention.size()} \")\n",
        "        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
        "        print(f\"values.size(): {values.size()}\")\n",
        "        out = self.linear_layer(values)\n",
        "        print(f\"out.size(): {out.size()}\")\n",
        "        return out"
      ],
      "metadata": {
        "id": "V2ZkE1XhsPgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply and use the class and function"
      ],
      "metadata": {
        "id": "ShgV3pUSvFh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 1024\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "\n",
        "batch_size = 30\n",
        "sequence_length = 5\n",
        "x = torch.randn( (batch_size, sequence_length, input_dim) )\n",
        "\n",
        "model = MultiheadAttention(input_dim, d_model, num_heads)\n",
        "out = model.forward(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqK6iHQQuMoc",
        "outputId": "e3cc8225-5ecb-4bb7-86ed-a3e120fa7157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x.size(): torch.Size([30, 5, 1024])\n",
            "qkv.size(): torch.Size([30, 5, 1536])\n",
            "qkv.size(): torch.Size([30, 5, 8, 192])\n",
            "qkv.size(): torch.Size([30, 8, 5, 192])\n",
            "q size: torch.Size([30, 8, 5, 64]), k size: torch.Size([30, 8, 5, 64]), v size: torch.Size([30, 8, 5, 64]), \n",
            "values.size(): torch.Size([30, 8, 5, 64]), attention.size:torch.Size([30, 8, 5, 5]) \n",
            "values.size(): torch.Size([30, 5, 512])\n",
            "out.size(): torch.Size([30, 5, 512])\n"
          ]
        }
      ]
    }
  ]
}