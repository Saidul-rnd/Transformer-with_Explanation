{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rGwN2PzXDT1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    # q: 30 x 8 x 200 x 64, k: 30 x 8 x 200 x 64, v: 30 x 8 x 200 x 64, mask 200 x 200\n",
        "    # deviding by d_k help to reduce the variance.\n",
        "    d_k = q.size()[-1]\n",
        "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k) # 30 x 8 x 200 x 200\n",
        "    print(f\"scaled.size() : {scaled.size()}\")\n",
        "    if mask is not None:\n",
        "        print(f\"-- ADDING MASK of shape {mask.size()} --\")\n",
        "        scaled += mask # 30 x 8 x 200 x 200\n",
        "    attention = F.softmax(scaled, dim=-1) # 30 x 8 x 200 x 200\n",
        "    values = torch.matmul(attention, v) # 30 x 8 x 200 x 64\n",
        "    return values, attention"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" its the feedforward network of the network \"\"\"\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, hidden)\n",
        "        self.linear2 = nn.Linear(hidden, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #  x: 30 x 200 x 512\n",
        "        x = self.linear1(x) #30 x 200 x 2048\n",
        "        print(f\"x after first linear layer: {x.size()}\")\n",
        "        x = self.relu(x) #30 x 200 x 2048\n",
        "        print(f\"x after relu layer: {x.size()}\")\n",
        "        x = self.dropout(x) #30 x 200 x 2048\n",
        "        print(f\"x after dropout layer: {x.size()}\")\n",
        "        x = self.linear2(x) #30 x 200 x 512\n",
        "        print(f\"x after 2nd linear layer: {x.size()}\")\n",
        "        return x #30 x 200 x 512"
      ],
      "metadata": {
        "id": "F7k7sMs4eVf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, parameters_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.parameters_shape=parameters_shape\n",
        "        self.eps=eps\n",
        "        self.gamma = nn.Parameter(torch.ones(parameters_shape)) # 512\n",
        "        self.beta =  nn.Parameter(torch.zeros(parameters_shape)) # 512\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # inputs : 30 x 200 x 512\n",
        "        dims = [-(i + 1) for i in range(len(self.parameters_shape))] # [-1]\n",
        "        print(f\"dims: {dims}\")\n",
        "        #calculate the mean\n",
        "        mean = inputs.mean(dim=dims, keepdim=True) #30 x 200 x 1\n",
        "        print(f\"Mean ({mean.size()})\")\n",
        "        #calculate the variance\n",
        "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True) # 30 x 200 x 512\n",
        "        #calculate the standard variation\n",
        "        std = (var + self.eps).sqrt() # 30 x 200 x 512\n",
        "        print(f\"Standard Deviation  ({std.size()})\")\n",
        "        y = (inputs - mean) / std # 30 x 200 x 512\n",
        "        print(f\"y: {y.size()}\")\n",
        "        out = self.gamma * y  + self.beta  # 30 x 200 x 512\n",
        "        print(f\"out: {out.size()}\")\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "7QUI1UAjXZdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" This is first Multiheadattention that all Q, K & V is from decoder \"\"\"\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.qkv_layer = nn.Linear(d_model , 3 * d_model) # 1536\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, sequence_length, d_model = x.size() # 30 x 200 x 512\n",
        "        print(f\"x.size(): {x.size()}\")\n",
        "        qkv = self.qkv_layer(x) # 30 x 200 x 1536\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "         #lets reshape to create multi-head (8-heads), so break the last layers accordingly 1536/8 = 192\n",
        "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim) # 30 x 200 x 8 x 192\n",
        "        print(f\"qkv after reshape .size(): {qkv.size()}\")\n",
        "        # Now, we have to permute as we want to make the opration between seq_len and dim_vec, so permute between num_head and seq_len\n",
        "        qkv = qkv.permute(0, 2, 1, 3) # 30 x 8 x 200 x 192\n",
        "        print(f\"qkv after permutation: {qkv.size()}\")\n",
        "        # we can to chunk the Q, K & V together, where we will chunk considering last dimention(dim= -1)\n",
        "        q, k, v = qkv.chunk(3, dim=-1) # q: 30 x 8 x 200 x 64, k: 30 x 8 x 200 x 64, v: 30 x 8 x 200 x 64\n",
        "        print(f\"q: {q.size()}, k:{k.size()}, v:{v.size()}\")\n",
        "        #lets call the scaled_dot_product for self-attention mechanism\n",
        "        values, attention = scaled_dot_product(q, k, v, mask) # values: 30 x 8 x 200 x 64\n",
        "        print(f\"values: {values.size()}, attention:{attention.size()}\")\n",
        "        # after completing the self-attention calculation, now we have to bring back to the original shape, which is multiply 8 x 64 and place it in last dimention\n",
        "        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim) # 30 x 200 x 512\n",
        "        print(f\"values after reshaping: {values.size()}\")\n",
        "        out = self.linear_layer(values) # 30 x 200 x 512\n",
        "        print(f\"out after passing through linear layer: {out.size()}\")\n",
        "        return out # 30 x 200 x 512"
      ],
      "metadata": {
        "id": "R42ccKdpeMA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" This is cross Multiheadattention that the Q is from Decoder and K & V is from the output of encoder \"\"\"\n",
        "class MultiHeadCrossAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        # we can keep the Q and KV separate here, as we will use only the Q from decoder and KV from encoder\n",
        "        self.kv_layer = nn.Linear(d_model , 2 * d_model) # 1024\n",
        "        self.q_layer = nn.Linear(d_model , d_model) # 512\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, y, mask=None):\n",
        "        batch_size, sequence_length, d_model = x.size() # 30 x 200 x 512\n",
        "        print(f\"x.size(): {x.size()}\")\n",
        "        kv = self.kv_layer(x) # 30 x 200 x 1024\n",
        "        print(f\"kv.size(): {kv.size()}\")\n",
        "        q = self.q_layer(y) # 30 x 200 x 512\n",
        "        print(f\"q.size(): {q.size()}\")\n",
        "        #lets reshape to create multi-head (8-heads), so break the last layers accordingly 512/8 = 64 & 1024/8 = 128\n",
        "        kv = kv.reshape(batch_size, sequence_length, self.num_heads, 2 * self.head_dim)  # 30 x 200 x 8 x 128\n",
        "        q = q.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)  # 30 x 200 x 8 x 64\n",
        "        # Now, we have to permute as we want to make the opration between seq_len and dim_vec, so permute between num_head and seq_len\n",
        "        kv = kv.permute(0, 2, 1, 3) # 30 x 8 x 200 x 128\n",
        "        q = q.permute(0, 2, 1, 3) # 30 x 8 x 200 x 64\n",
        "        # we can to chunk the K & V together, where we will chunk considering last dimention(dim= -1)\n",
        "        k, v = kv.chunk(2, dim=-1) # K: 30 x 8 x 200 x 64, v: 30 x 8 x 200 x 64\n",
        "        # for the multihead-cross attention, the mask is not needed. so the mask = 0 here\n",
        "        values, attention = scaled_dot_product(q, k, v, mask) #  30 x 8 x 200 x 64\n",
        "        print(f\"values: {values.size()}, attention:{attention.size()}\")\n",
        "        # after completing the self-attention calculation, now we have to bring back to the original shape, which is multiply 8 x 64 and place it in last dimention\n",
        "        values = values.reshape(batch_size, sequence_length, d_model) #  30 x 200 x 512\n",
        "        out = self.linear_layer(values)  #  30 x 200 x 512\n",
        "        print(f\"out after passing through linear layer: {out.size()}\")\n",
        "        return out  #  30 x 200 x 512"
      ],
      "metadata": {
        "id": "f11xw6PmYHFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" This is the decoderlayer of the transform \"\"\"\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm3 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x, y, decoder_mask):\n",
        "        #_y is the residual connection\n",
        "        _y = y # 30 x 200 x 512\n",
        "        print(\"MASKED SELF ATTENTION\")\n",
        "        y = self.self_attention(y, mask=decoder_mask) # 30 x 200 x 512\n",
        "        print(\"DROP OUT 1\")\n",
        "        # drop_out is needed for avoding overfitting\n",
        "        y = self.dropout1(y) # 30 x 200 x 512\n",
        "        print(\"ADD + LAYER NORMALIZATION 1\")\n",
        "        y = self.norm1(y + _y) # 30 x 200 x 512\n",
        "\n",
        "        _y = y # 30 x 200 x 512\n",
        "        print(\"CROSS ATTENTION\")\n",
        "        y = self.encoder_decoder_attention(x, y, mask=None) #30 x 200 x 512\n",
        "        print(\"DROP OUT 2\")  #30 x 200 x 512\n",
        "        y = self.dropout2(y)\n",
        "        print(\"ADD + LAYER NORMALIZATION 2\")\n",
        "        y = self.norm2(y + _y)  #30 x 200 x 512\n",
        "\n",
        "        _y = y  #30 x 200 x 512\n",
        "        print(\"FEED FORWARD 1\")\n",
        "        y = self.ffn(y) #30 x 200 x 512\n",
        "        print(\"DROP OUT 3\")\n",
        "        y = self.dropout3(y) #30 x 200 x 512\n",
        "        print(\"ADD + LAYER NORMALIZATION 3\")\n",
        "        y = self.norm3(y + _y) #30 x 200 x 512\n",
        "        return y #30 x 200 x 512"
      ],
      "metadata": {
        "id": "-67iyjoNXqF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" This SequentialDecoder is for generating the output. here the purpose of the\n",
        "for loop is to pass the output generated by the decoder as input of the decoder for the next iteration.\n",
        "Here, the X(output from encoder) will not change, it will be always same. \"\"\"\n",
        "class SequentialDecoder(nn.Sequential):\n",
        "    def forward(self, *inputs):\n",
        "        x, y, mask = inputs\n",
        "        for module in self._modules.values():\n",
        "            y = module(x, y, mask) #30 x 200 x 512\n",
        "        return y\n",
        "\n",
        "\"\"\" This ithe decoder class where the whole decoder modules executed\"\"\"\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
        "                                          for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, y, mask):\n",
        "        #x : 30 x 200 x 512  #x = English sentence\n",
        "        #y : 30 x 200 x 512 # y = french sentence\n",
        "        #mask : 200 x 200, max_seq_len X max_seq_len\n",
        "        y = self.layers(x, y, mask)\n",
        "        return y #30 x 200 x 512"
      ],
      "metadata": {
        "id": "OhDnKonscbTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512 # each word is represented by 512 dimentional vector\n",
        "num_heads = 8 # number of heads in multi-head attention\n",
        "drop_prob = 0.1 #drop probability in linear layer\n",
        "batch_size = 30 # the batch size for training, which help us for parallal training\n",
        "max_sequence_length = 200 #maximum sequence length of input\n",
        "ffn_hidden = 2048 # for the linear layer, there will be hidden layer. so its the size of that hidden layer\n",
        "num_layers = 5 # this is the number of decoder layer\n",
        "\n",
        "\"\"\" here, x is the output from the encoder and y is the input of decoder with positional encoding\n",
        "for the purpose here, we just take randn value for calculation, but in reality it will word embedding vectors \"\"\"\n",
        "x = torch.randn( (batch_size, max_sequence_length, d_model) ) # English sentence positional encoded\n",
        "y = torch.randn( (batch_size, max_sequence_length, d_model) ) # Kannada sentence positional encoded\n",
        "# Add masking to prevent to look at the next words\n",
        "mask = torch.full([max_sequence_length, max_sequence_length] , float('-inf'))\n",
        "mask = torch.triu(mask, diagonal=1)\n",
        "# pass all the parameters to the Decoder function here,\n",
        "decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)\n",
        "out = decoder(x, y, mask) #model forward pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J220KAOmXxwj",
        "outputId": "c4190fd6-7dbb-4221-bd17-e565ed3addd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MASKED SELF ATTENTION\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv after reshape .size(): torch.Size([30, 200, 8, 192])\n",
            "qkv after permutation: torch.Size([30, 8, 200, 192])\n",
            "q: torch.Size([30, 8, 200, 64]), k:torch.Size([30, 8, 200, 64]), v:torch.Size([30, 8, 200, 64])\n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "-- ADDING MASK of shape torch.Size([200, 200]) --\n",
            "values: torch.Size([30, 8, 200, 64]), attention:torch.Size([30, 8, 200, 200])\n",
            "values after reshaping: torch.Size([30, 200, 512])\n",
            "out after passing through linear layer: torch.Size([30, 200, 512])\n",
            "DROP OUT 1\n",
            "ADD + LAYER NORMALIZATION 1\n",
            "dims: [-1]\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "CROSS ATTENTION\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "kv.size(): torch.Size([30, 200, 1024])\n",
            "q.size(): torch.Size([30, 200, 512])\n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values: torch.Size([30, 8, 200, 64]), attention:torch.Size([30, 8, 200, 200])\n",
            "out after passing through linear layer: torch.Size([30, 200, 512])\n",
            "DROP OUT 2\n",
            "ADD + LAYER NORMALIZATION 2\n",
            "dims: [-1]\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "FEED FORWARD 1\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after relu layer: torch.Size([30, 200, 2048])\n",
            "x after dropout layer: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "DROP OUT 3\n",
            "ADD + LAYER NORMALIZATION 3\n",
            "dims: [-1]\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "MASKED SELF ATTENTION\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv after reshape .size(): torch.Size([30, 200, 8, 192])\n",
            "qkv after permutation: torch.Size([30, 8, 200, 192])\n",
            "q: torch.Size([30, 8, 200, 64]), k:torch.Size([30, 8, 200, 64]), v:torch.Size([30, 8, 200, 64])\n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "-- ADDING MASK of shape torch.Size([200, 200]) --\n",
            "values: torch.Size([30, 8, 200, 64]), attention:torch.Size([30, 8, 200, 200])\n",
            "values after reshaping: torch.Size([30, 200, 512])\n",
            "out after passing through linear layer: torch.Size([30, 200, 512])\n",
            "DROP OUT 1\n",
            "ADD + LAYER NORMALIZATION 1\n",
            "dims: [-1]\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "CROSS ATTENTION\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "kv.size(): torch.Size([30, 200, 1024])\n",
            "q.size(): torch.Size([30, 200, 512])\n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values: torch.Size([30, 8, 200, 64]), attention:torch.Size([30, 8, 200, 200])\n",
            "out after passing through linear layer: torch.Size([30, 200, 512])\n",
            "DROP OUT 2\n",
            "ADD + LAYER NORMALIZATION 2\n",
            "dims: [-1]\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "FEED FORWARD 1\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after relu layer: torch.Size([30, 200, 2048])\n",
            "x after dropout layer: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "DROP OUT 3\n",
            "ADD + LAYER NORMALIZATION 3\n",
            "dims: [-1]\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "MASKED SELF ATTENTION\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv after reshape .size(): torch.Size([30, 200, 8, 192])\n",
            "qkv after permutation: torch.Size([30, 8, 200, 192])\n",
            "q: torch.Size([30, 8, 200, 64]), k:torch.Size([30, 8, 200, 64]), v:torch.Size([30, 8, 200, 64])\n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "-- ADDING MASK of shape torch.Size([200, 200]) --\n",
            "values: torch.Size([30, 8, 200, 64]), attention:torch.Size([30, 8, 200, 200])\n",
            "values after reshaping: torch.Size([30, 200, 512])\n",
            "out after passing through linear layer: torch.Size([30, 200, 512])\n",
            "DROP OUT 1\n",
            "ADD + LAYER NORMALIZATION 1\n",
            "dims: [-1]\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "CROSS ATTENTION\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "kv.size(): torch.Size([30, 200, 1024])\n",
            "q.size(): torch.Size([30, 200, 512])\n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values: torch.Size([30, 8, 200, 64]), attention:torch.Size([30, 8, 200, 200])\n",
            "out after passing through linear layer: torch.Size([30, 200, 512])\n",
            "DROP OUT 2\n",
            "ADD + LAYER NORMALIZATION 2\n",
            "dims: [-1]\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "FEED FORWARD 1\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after relu layer: torch.Size([30, 200, 2048])\n",
            "x after dropout layer: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "DROP OUT 3\n",
            "ADD + LAYER NORMALIZATION 3\n",
            "dims: [-1]\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "MASKED SELF ATTENTION\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv after reshape .size(): torch.Size([30, 200, 8, 192])\n",
            "qkv after permutation: torch.Size([30, 8, 200, 192])\n",
            "q: torch.Size([30, 8, 200, 64]), k:torch.Size([30, 8, 200, 64]), v:torch.Size([30, 8, 200, 64])\n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "-- ADDING MASK of shape torch.Size([200, 200]) --\n",
            "values: torch.Size([30, 8, 200, 64]), attention:torch.Size([30, 8, 200, 200])\n",
            "values after reshaping: torch.Size([30, 200, 512])\n",
            "out after passing through linear layer: torch.Size([30, 200, 512])\n",
            "DROP OUT 1\n",
            "ADD + LAYER NORMALIZATION 1\n",
            "dims: [-1]\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "CROSS ATTENTION\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "kv.size(): torch.Size([30, 200, 1024])\n",
            "q.size(): torch.Size([30, 200, 512])\n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values: torch.Size([30, 8, 200, 64]), attention:torch.Size([30, 8, 200, 200])\n",
            "out after passing through linear layer: torch.Size([30, 200, 512])\n",
            "DROP OUT 2\n",
            "ADD + LAYER NORMALIZATION 2\n",
            "dims: [-1]\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "FEED FORWARD 1\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after relu layer: torch.Size([30, 200, 2048])\n",
            "x after dropout layer: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "DROP OUT 3\n",
            "ADD + LAYER NORMALIZATION 3\n",
            "dims: [-1]\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "MASKED SELF ATTENTION\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "qkv.size(): torch.Size([30, 200, 1536])\n",
            "qkv after reshape .size(): torch.Size([30, 200, 8, 192])\n",
            "qkv after permutation: torch.Size([30, 8, 200, 192])\n",
            "q: torch.Size([30, 8, 200, 64]), k:torch.Size([30, 8, 200, 64]), v:torch.Size([30, 8, 200, 64])\n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "-- ADDING MASK of shape torch.Size([200, 200]) --\n",
            "values: torch.Size([30, 8, 200, 64]), attention:torch.Size([30, 8, 200, 200])\n",
            "values after reshaping: torch.Size([30, 200, 512])\n",
            "out after passing through linear layer: torch.Size([30, 200, 512])\n",
            "DROP OUT 1\n",
            "ADD + LAYER NORMALIZATION 1\n",
            "dims: [-1]\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "CROSS ATTENTION\n",
            "x.size(): torch.Size([30, 200, 512])\n",
            "kv.size(): torch.Size([30, 200, 1024])\n",
            "q.size(): torch.Size([30, 200, 512])\n",
            "scaled.size() : torch.Size([30, 8, 200, 200])\n",
            "values: torch.Size([30, 8, 200, 64]), attention:torch.Size([30, 8, 200, 200])\n",
            "out after passing through linear layer: torch.Size([30, 200, 512])\n",
            "DROP OUT 2\n",
            "ADD + LAYER NORMALIZATION 2\n",
            "dims: [-1]\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "out: torch.Size([30, 200, 512])\n",
            "FEED FORWARD 1\n",
            "x after first linear layer: torch.Size([30, 200, 2048])\n",
            "x after relu layer: torch.Size([30, 200, 2048])\n",
            "x after dropout layer: torch.Size([30, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
            "DROP OUT 3\n",
            "ADD + LAYER NORMALIZATION 3\n",
            "dims: [-1]\n",
            "Mean (torch.Size([30, 200, 1]))\n",
            "Standard Deviation  (torch.Size([30, 200, 1]))\n",
            "y: torch.Size([30, 200, 512])\n",
            "out: torch.Size([30, 200, 512])\n"
          ]
        }
      ]
    }
  ]
}